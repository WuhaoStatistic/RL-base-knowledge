### Temporal-Difference Learning

时间差分学习比起蒙特卡洛效率更高。蒙特卡洛方法要求我们的情景必须是episodic，TD方法没有这个要求，甚至连续的state都是可以的。
在蒙特卡洛方法中，我们有如下更新公式
$$
    V(S_t) <-\ V(S_t)+\frac{1}{N(S_t)}(G_t-V(S_t))
$$
再次强调这里的`step size`$\frac{1}{N(S_t)}$可以换成其他数值。
而在TD中，我们引入两个新的概念
1 TD target
2 TD error

TD target = $R_{t+1}+\gamma V(S_{t+1})$
TD error = $R_{t+1}+\gamma V(S_{t+1}) - V(S_t)$

注意这里使用的是V而不是$V_\pi$,因为我们在这里是没有一个确定的MDP的。这里的V是我们到目前为止所能得到的最好的状态价值函数。而我们的更新公式
$$
    V(S_t) <-\ V(S_t)+\alpha(R_{t+1}+\gamma V(S_{t+1}) - V(S_t))
$$
每当状态进行转移时，我们采用“动态规划”。依照我们之前学习所获得的“知识”来更新当前状态的状态价值函数，这无疑比起蒙特卡洛提升了许多的效率。但是，因为我们采用的TD target并不是当前episode真实获得的$G_t$,因此，TD方法中的奖励是对真实奖励的有偏估计

### TD($\lambda$)方法

可能会有一个疑惑，为什么TD方法走一步就要更新一下，为什么我不能走两步更新一下，或者走三步更新一次呢？答案是可以的。接下来 我们要展开TD($\lambda$)方法。注意这里的$\lambda$并不是我们走多少步，而是一个权重参数。

首先给出下面给出TD($\lambda$)的基础**n-step return**的定义和学习方式。可以看到n=1时就是TD(0)算法，n=$\infin$就是MC算法。
![1](/图片/5.png)

更进一步，我是不是能够在同一个状态同时运行很多不同的n值（或者走很多步，每一步我都记录下来），然后将这些结果综合平均起来，使我的最终结果更加robust？答案是可以的，这就是$\mathbf{\lambda -return}$算法。

![2](/图片/11.png)
在$\mathbf{\lambda -return}$算法中，不同时刻的return被赋予了不同的权重，由等比数列求和公式（首项为$1-\lambda$,比例为$\lambda$）可知,权重总和为1。我们将获得的$G_t^\lambda$代替原来的$G_t^{(n)}$增加模型的鲁棒性。注意，这里我们考虑的始终为走到终点terminate的情况。

上述描述的是**Forward-view TD($\lambda$)**
![2](/图片/12.png)
它的特点是类似于MC算法，只能从完整的episodes中学习。
他的计算量相当大，因为每一步我们都需要走到结尾才能回头更新，因此它也属于offline-learning。

另外一种是**Backward view TD($\lambda$)**
![3](/图片/13.png)

**Eligibility traces**

在时刻t时，拿到了Rt构建了return，然后利用这个最新构建出来的return去更新t时刻之前的所有状态对应的值函数。远离时刻t的状态的值函数，其对应更新幅度会被乘以一个指数衰减的权重。因为我们认为对它的更新其置信度较低，更新幅度较小。该权重由资格迹（Eligibility Traces）表示。
$$\begin{aligned}
   E_0(s) &=0 \\
E_t(s) &= \gamma \lambda E_{t-1}(s)+\mathbf{1}(S_t=s) 
\end{aligned}
$$

Backward view TD ($\lambda$)提供了一种online-learning的方法，他在每一步都会进行参数的更新。**我们一般说的$TD(\lambda)$指的就是Backward view TD($\lambda$)**

这里有一个理论，对于offline的数据（即完整的episode）而言，online和offline是相同的
![4](/图片/14.png)

当$\lambda=0$时，就完完全全是TD(0)算法，因为只有当前状态的$E_t$不为0。
当$\lambda=1$时，该算法近似MC算法，因为最终一整个eposide的更新是和MC相同的。而forward-view TD(1)和MC是完全相同的。

### MC V.S TD

**MC方法是无偏的 而TD方法是有偏的。**

在MC方法中，我们是通过完整的路径来更新中途的每一个状态。而我们的return $G_t$就是$v_\pi(S_t)$的无偏但有噪声估计。

在TD中，我们并不存在$G_t$,相对的，我们用TD target作为$G_t$的估计，这显然是一个有偏估计,因为我们用“对下一状态的状态价值函数的估计“ 来代替真实的每一步的奖励。

**MC方法更新迭代方差更大 而TD更新方差更小。**

MC方法每一次更新涉及到非常多的随机动作，状态转移和奖励，而TD方法在每一次更新只涉及一个状态的动作转移和奖励，方差会小很多。

**MC方法对初始值不敏感 TD对初始值敏感。**

MC方法更新的时候已经走了一遍完整的流程，我们获得的奖励和状态的初始值是无关的，因此随着时间的推进，初始值的影响会越来越小‘；而TD更新状态价值函数的时候是用的下一状态的状态价值函数，因此在初期，更新的时候很容易会被初始值影响。