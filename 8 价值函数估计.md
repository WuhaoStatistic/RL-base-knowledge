## value function approximation 

在这一章节，我们开始考虑实际运用中的强化学习。在Q-learning或者之前的MDP中，我们都是 构建了一个足够大的lookup-table 来获取所有$v_\pi(s)$和$q_\pi(s,a)$。

而当问题规模变得很大，而且我们需要generalize的时候就会出现问题。state，action的增长对于表大小的扩张是指数级别的影响。

这时候，我们推出一种利用函数近似的方法。这里的函数可以是任何形式的函数包括可导的深度神经网络，线性模型等；或者不可导的决策树等。
$$
f_v(s,w) \approx v_\pi(s) \\
f_q(s,a,w)\approx q_\pi(s,a)
$$

在这里，我们主要讨论那些可以求导的方法。在监督学习的随机梯度下降中，我们经常用MSE来作为我们的error function，然后对参数求导来更新参数。但是在强化学习中，我们并没有一个真实的value function，一切都得依靠学习来获得。那么我们该怎么做呢？

### 状态价值函数
#### 1 MC方法

想起我们之前学过的蒙特卡洛方法，我们会记录从头到尾的所有状态，然后每当我们走完一次旅程时，我们会用这次旅程获得的return（也就是$G_t$）来更新状态价值函数。在这里我们一样可以用这种方法。**下面的式子是我们采用MC方法，将$G_t$作为真实$v_\pi(s)$的无偏估计得出的参数更新公式**，注意这里并不是error function，已经是导数，下式对应的原始error fucntion为MSE
$$
\triangle \mathbf{w}=\alpha(G_t-\hat v(S_t,\mathbf{w}))\triangledown_\mathbf{w} \hat v(S_t,\mathbf{w})
$$

#### 2 TD(0)方法

在time difference方法中，我们采用了动态规划的思想，**当我们在状态s，即将进入状态s+1时，我们会用TD target作为我们对当前状态s的$G_t$的有偏估计**。如果我们的error function仍然采用MSE的话，我们会有如下的参数更新公式
$$
\triangle \mathbf{w}=\alpha(\underbrace{R_{t+1}+\gamma \hat v(S_{t+1},\mathbf{w})}_{TD\ target}-\hat v(S_t,\mathbf{w}))\triangledown_\mathbf{w} \hat v(S_t,\mathbf{w})
$$

#### 3 TD($\lambda$)方法

和上面类似，只不过我们将TD target换成了TD($\lambda$)的TD target
$$
\triangle \mathbf{w}=\alpha(G_t^{(\lambda)}-\hat v(S_t,\mathbf{w}))\triangledown_\mathbf{w} \hat v(S_t,\mathbf{w})
$$
`hints`: $\alpha$`为步长，或学习率`

### 动作价值函数
我们可以将同样的套路用在动作价值函数上
![1](/图片/7.png)