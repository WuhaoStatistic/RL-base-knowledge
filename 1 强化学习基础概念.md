要点1

强化学习 和 监督学习的区别：

强化学习是基于**对动作评估**来完成的，他的反馈是完全基于动作执行的，且训练时基本没有显式的表示出哪一个动作更好哪一个动作更坏的结论。监督学习是依靠指令进行的，他一直有一个绝对最优的指令。他的反馈和动作是独立的。



首先提到一个最基本的公式，每一个action的期望奖励Q是一个不断迭代更新的结果，那么再第n步

$$
Q_n\ =\ \frac{R_1+R_2+.....+R_{n-1}}{n-1}
$$

这个公式可以写成递归的形式

$$
Q_{n+1}\ =\ Q_n+\frac{1}{n}[R_n-Q_n]
$$

对于Q1，我们一般有一个默认的初始值。这一种写法实际上是 **[Target - OldEstimate]**。这里的 **n分之1** 我们成为 step-size 也就是步长，对应于一般机器学习任务中，可以称之为学习率。在更general的情况下，我们可以将步长写成$\alpha$.

$$
\begin{aligned}
Q_{n+1}&=Q_n+\alpha [R_n-Q_n] \\
&=\alpha R_n+(1-\alpha)Q_n \\
&=\alpha R_n+(1-\alpha)[\alpha R_{n-1}+(1-\alpha)Q_{n-1}] \\
&=(1-\alpha)^n Q_1+\sum_{i=1}^n \alpha(1-\alpha)^{n-i} R_i
\end{aligned}
$$

上面这个公式很有意思 记录一下，我们可以发现最底下
$$
(1-\alpha)^n Q_1+\sum_{i=1}^n \alpha(1-\alpha) =1
$$

然而有些时候我们的学习率并不是固定的，他可能是一个关于$\alpha$的函数，$\alpha_n(a)$ 。在这种情况下，我们如何能保证我们选取的函数能够最终收敛（只有收敛了我们才能认为学得了一个可行的policy）。随机优化定义了如下两个规则来确保收敛性：
$$
\sum_{i=1}^\infin \alpha_n(\alpha) = \infin\ \ and\ \   \sum_{i=1}^\infin \alpha_n^2(\alpha) < \infin
$$
第一个式子表示了我们有足够多的步骤去覆盖我们初始化数值对policy带来的影响，而第二个式子表示了我们的step在scale上足够小能满足收敛。我们最开始的$\frac{1}{n}$就算一个例子。

强化学习能分为两大类 online learning 和offline lkearning。

online RL：在线强化学习：

学习过程中，智能体需要和环境进行交互。并且在线强化学习可分为on-policy RL和off-policy RL。on-policy采用的是当前策略搜集的数据训练模型，每条数据仅使用一次。off-policy训练采用的数据不需要是当前策略搜集的。

offline RL:离线强化学习：
学习过程中，不与环境进行交互，只从dataset中直接学习，而dataset是采用别的策略收集的数据，并且采集数据的策略并不是近似最优策略。

Off-policy RL算法有：Q-learning,DQN,DDPG,SAC,etc.
On-policy RL算法有：REINFORCE,A3C,PPO,etc.

Off-line RL 和Imitation Learning的区别：Off-line RL中数据包括奖励，IL中数据不包括奖励。Off-line RL不要求数据是近似最优策略的得到的，IL中的专家数据基于得到搜集专家数据的策略是近似最优策略的假设。