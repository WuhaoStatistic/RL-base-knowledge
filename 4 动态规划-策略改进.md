## 策略改进
书接上回，上回我们提到了策略评估的解析解和迭代解。这一章我们介绍策略改进。

策略改进定理

给定$\pi,\pi\prime$,如果对于任意$s \in S$, $q_\pi(s,\pi\prime(s))\geq \mathbb{v}_\pi(s)$,那么则有任意$s \in S$，$\mathbb{v}_{\pi \prime}(s) \geq \mathbb{v}_\pi(s)$

![4](/图片/4.png)

第一个等号是按照定义公式展开，其中我们用$S_{t+1}$表示采取$\pi \prime(s)$中的动作之后的下一个状态。注意这里我们不能在期望右下角标上$\pi$,因为$R_{t+1}$在给定了S之后，只由$\pi \prime$控制。$v_\pi$是由$\pi$来控制。

第二个等号我们是符号上的改写，我们发现$A_t$是一个由$\pi \prime$控制的随机变量。所以直接将$\pi \prime$写在期望边上，条件里去掉和$\pi \prime$相关的$A_t$。


我们可以将我们的条件 $q_\pi(s,\pi\prime(s))\geq \mathbb{v}_\pi(s)$套入第二个等号,只不过这里将s换成s+1，因为我们的假设是对于任意s都成立$q_\pi(s+1,\pi\prime(s+1))\geq \mathbb{v}_\pi(s+1)$，就得到了第一个不等号。

第二个不等号重复了第一个的行为。长此以往，我们可以无限进行这样的步骤，最终得到的结果就是$\mathbb{v}_{\pi \prime}(s)$的定义。





























