### 蒙特卡洛方法

记得我们在强化学习基础概念中提到过一个**incremental updates**
$$
Q_{n+1}\ =\ Q_n+\frac{1}{n}[R_n-Q_n]
$$
其中$Q_n$是当前轮次的价值函数，$R_n$是当前轮次的奖励。每一轮，我们都用这样的方法去更新价值函数。在蒙特卡洛方法中，我们借鉴了这个思路。

蒙特卡洛方法使用的前提是：

我们的情景必须是eposidic的，我的理解是离散的有限状态的。

初始化：
    我们随机初始化每一个状态的价值函数$V(S_t)$。
    我们给所有状态附上计数器并初始化$N(S_t)=$0。

循环：
    当我们从某一个状态S开始经历S1,S2,S3,S4....直到结束时，每经历一个状态时，将其对应的计数器加1.结束时我们拿到一个奖励$G_t$，更新当前轮次所有经历过的状态的价值函数。
    $$
    V(S_t) <-\ V(S_t)+\frac{1}{N(S_t)}(G_t-V(S_t))
    $$

经过足够多的轮次，我们会对所有的状态有一个价值函数，根据大数定理，在足够多的轮次条件下，我们得到的结果就逼近真实结果。

我们可以看到蒙特卡洛方法并不需要我们有一个确定的MDP和transition matrix，只需要不断地从尝试中获取信息，迭代结果即可。

事实上，我们也可以通过改变`step size` $\frac{1}{N(S_t)}$ 来达到遗忘的目的。如果始终保持对$\frac{1}{N(S_t)}$的追踪，说明我们每一轮的更新都会用到以前的经历（计数器是不断叠加的）。我们可以将其换成其他的数值，来达到每一次更新不依赖于之前的结果，也就是“遗忘”。