动态规划（Dynamic programming）在马尔可夫决策背景下有自己的应用。

动态规划可以分为**策略迭代**和**价值迭代**，策略迭代的过程如下
$$ \pi \underset{策略评估}{---->}q_\pi(s,a) \underset{策略改进}{---->}\pi\prime$$

每一次迭代过程经历策略评估和策略改进两部分，由初始策略得出一个q，再通过q得出一个新的策略使得新策略优于原策略。 这个过程就是一次策略迭代。这一章主要了解策略评估

## 策略评估-解析解

在策略评估中，我们已知动态特性（已知$p(s\prime,r|s,a)$）,给定一个$\pi$,求$\mathbb{v}_\pi$。

假定我们的状态空间中一共由|S|个元素，那么我们可以设定价值函数总体为一个向量。

$$\mathbb{v}_\pi=
\begin{pmatrix}
\mathbb{v}_\pi(s_1) \\
\mathbb{v}_\pi(s_2) \\
\mathbb{v}_\pi(s_3) \\
. \\
\mathbb{v}_\pi(s_{|s|}) 
\end{pmatrix}_{|S|*1}$$

从上一节马尔可夫决策过程中，我们有关于状态价值函数的贝尔曼期望方程。
$$\begin{aligned}
\mathcal{v}_\pi(s) &= \sum_{a \in A}\pi(a|s)(R_s^a+\gamma\sum_{s\prime,r\prime}P_{ss\prime}^a \mathcal{v}_\pi(s\prime)) \\
&= \sum_a \pi(a|s) \sum_{s\prime,r}p(s\prime,r|s,a)r+\gamma\sum_a\pi(a|s)\sum_{s\prime,r}p(s\prime,r|s,a)\mathbb{v}_\pi(s\prime)
\end{aligned}$$
上式中，我们将$R_s^a$写回r，$P_{ss\prime}^a$写回状态转移函数的定义$p(s\prime,r|s,a)$。上式中的最后一行我们可以将加号前半部分和加号后半部分分为两部分考虑。
第一部分中
$$
\sum_{s\prime,r}p(s\prime,r|s,a)r
$$
对于$s\prime$的求和依照联合概率公式可以写为
$$
\sum_{r}p(r|s,a)r
$$
而上式是一个关于参数s，a的函数，我们将其记为$r(s,a)$那么第一部分可以写为
$$
\sum_a \pi(s,a)r(s,a)
$$
同理对a求和之后结果与a无关，因此可以将最终结果记为$r_\pi(s)$.
因为我们总共有|S|个状态，因此$r_\pi(s)$也可以表示为一个列向量
$$\mathbb{r}_\pi=
\begin{pmatrix}
\mathbb{r}_\pi(s_1) \\
\mathbb{r}_\pi(s_2) \\
\mathbb{r}_\pi(s_3) \\
. \\
\mathbb{r}_\pi(s_{|s|}) 
\end{pmatrix}_{|S|*1}$$
对于加号后半段
$$\begin{aligned}
&=\gamma\sum_a\pi(a|s)\sum_{s\prime,r}p(s\prime,r|s,a)\mathbb{v}_\pi(s\prime) \\
&=\gamma\sum_{s\prime}\sum_a\pi(a|s)\mathbb{p}(s\prime|s,a)\mathbb{v}_\pi(s\prime)
\end{aligned}$$
加和前移的原因是$\pi(a|s)$中不包含$s\prime$。
$$
将 \sum_a\pi(a|s)\mathbb{p}(s\prime|s,a)记为P_\pi(s,s\prime)
$$
那么原式可以写为
$$
\gamma\sum_{s\prime}P_\pi(s,s\prime)\mathbb{v}_\pi(s\prime)
$$
上面这个式子也可以写成矩阵的表达形式，考虑$P_\pi(s,s\prime)$,这个矩阵应该是一个$|S|*|S|$的规模（下面记为$P_\pi$），而$\mathbb{v}_\pi(s\prime)$是一个$|S|*1$的列向量,也就是我们开头提到的状态价值函数列向量。
所以，我们的状态价值函数可以写成矩阵运算的形式
$$
\mathbb{v}_\pi=r_\pi+\gamma P_\pi \mathbb{v}_\pi
$$
解得状态价值函数的解析解
$$
\mathbb{v}_\pi=(I- P_\pi)^{-1} \mathbb{r}_\pi
$$
这个解析解中 矩阵P是一个SXS的矩阵，r是一个S   X1的矩阵，因此总体的计算复杂度为$\mathbb{O}(S^3)$,在很大的状态空间中，这个计算复杂度是难以接受的，因此我们提出策略评估的迭代解。

## 策略评估-迭代解
原公式中，我们有
$$
\mathbb{v}_\pi(s)=\sum_a \pi(a|s) \sum_{s\prime,r}p(s\prime,r|s,a)r+\gamma\sum_a\pi(a|s)\sum_{s\prime,r}p(s\prime,r|s,a)\mathbb{v}_\pi(s\prime)
$$
从迭代的角度去思考，我们所要求的$\mathbb{v}_\pi$实际上可以表示为无限迭代至收敛的结果。
$$
\mathbb{v}_{k+1}(s)=\sum_a \pi(a|s) \sum_{s\prime,r}p(s\prime,r|s,a)r+\gamma\sum_a\pi(a|s)\sum_{s\prime,r}p(s\prime,r|s,a)\mathbb{v}_k(s\prime)
$$
$$
\underset{k->\infin}{lim} \mathbb{v}_k = \mathbb{v}_\pi
$$