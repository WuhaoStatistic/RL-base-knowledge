策略梯度引入的原因：
### 基础&引入
value based 方法 有以下几点不足：

1 对于连续型动作处理能力不足。 

2 对受限状态下的问题能力处理不足，我们使用特征来描述状态空间中的某一状态时，有可能因为建模的原因使得本来不同的状态出现近似的特征向量

3 无法解决随机策略问题，例如在游戏石头剪刀布中，最好的策略是均匀随机策略，但是value based强化学习方法通常对应一个最优的确定性策略。

参考[论文](https://homes.cs.washington.edu/~todorov/courses/amath579/reading/PolicyGradient.pdf)，我们得出以下推导公式证明策略梯度的可行性和方法。

首先先给出一些记号，方便书写


$P_{ss\prime}^a =Pr\{s_{t+1}=s\prime|s_t=s,a_t=a\}$
$R_s^a=E\{r_{t+1}|s_t=s,a_t=a\}, \forall s,s\prime \in \mathbf{S},a \in \mathbf{A}$ 
$\pi(s,a,\theta)=Pr\{a_t=a,s_t=s,\theta\}$
$J(\theta)==V\pi(s_0)$
这里的$\theta$表示function approximation中的参数，我们假设我们的策略对$\theta$是可导的。而我们需要优化的目标函数在起始点为固定或满足某一分布（且非连续）时就是s0状态的价值函数。如果是其他情况其实也没关系，论文中给出了证明最终他们都是归一到同一个表达式

另外还有一点值得注意的是，对于离散动作空间，我们经常用softmax来表征这个概率分布，而连续工作空间下用高斯分布来表征。
![1](/图片/20.png)
上面图中的两个都是经过参数化的，对于softmax，指数分布族的指数是一个关于$s,a,\theta$的函数，而高斯分布的均值和方差则是一个关于$s,\theta$的函数。

在开始之前我们首先需要明确马尔可夫平稳状态的分布。
在平稳时期，马尔可夫链的状态概率分布可以由状态出现的次数做无偏估计。
我们先定义一个
$Pr(s_0->s,k,\pi)$表示在策略$\pi$语境下，从s0开始走k步之后状态为s的概率。那么在一个完整的T时间之内，每个状态s出现的次数，特别的$Pr(s_0->s_0,0,\pi)=1$
$$n(s) = \sum_{k=0}^{T-1}Pr(s_0->s,k,\pi)$$  
然后概率为
$$
\eta(s)=\frac{n(s)}{\sum_{s\prime}(s\prime)}
$$
理解了上面的所有内容，就可以开始策略梯度的学习。
### 策略梯度定理及蒙特卡洛形式应用
策略梯度是对我们的J求导，而我们在之前已经做过定义J实际上是初始状态的价值函数。因此,
$$
\bigtriangledown J(\theta)=\bigtriangledown V_\pi(s_0)
$$
我们先将s0用s来表示，这样书写也更加方便，最终再将s0代入。[精准空降](https://www.bilibili.com/video/BV18u411Q76q?t=1587.5)方便双屏对照着看
![1](/图片/21.png)


$q_\pi(s,a)$是s状态下动作a的价值函数，因此它等于该状态下对所有动作和所有动作对应到下一个状态$s\prime$的价值函数的求和，在求和式中对r求一下边缘概率分布（因为后面的部分不包括r只有$s\prime$），就得到了没有r的求和式子。然后我们知道$p(s\prime,r|s,a)$是马尔可夫链的动态特性，这和我们的策略是无关的，因为他是由环境提供的。所以对这个乘机的梯度实际上只需要对后面的部分求梯度就可以了。
使用下面这个简单的例子证明
![1](/图片/3.jpg)
3个状态2个奖励的情况下，可以写出所有的式子。式1因为式子中只包含了动态特性和奖励，这些都是环境超参数，和我们的策略参数$\theta$无关，因此梯度为0。而式2中是求了一下边缘概率分布，因此之前的是对的。

在第三行将$v_\pi(s\prime)$再一次展开。第四行蓝色部分的转写用到了我们前面提到的概率式子，这里可以加上我们说的特别的情况，从自身走0步到自身的概率为1，到其他状态的概率为0。


绿色部分的转写首先将对$s\prime$的求和提到了前面，这里的考量可以用一个技巧，就是将求和号看成是编程里的for循环，这样一眼就可以看出这是可以提到前面的是对的。绿色部分

第三行我们已经得到了一个递推式（从v(s)->$v(s\prime)$）。这个递推式是可以无限延展下去的，每延展一步相对应的就是我们在实际中走了一个时间步，即做了一个动作。那么考虑从状态s开始，将这个式子无限延展下去，那么我们的梯度就是每一个其他状态的梯度乘上一个概率值，这个概率值就是从状态s开始走k步到达那个那个状态的概率。因为从黑色第三行可以看到v(s)和$v_\pi(s\prime)$是求和的对应关系。$v_\pi(s)$是一个东西加上一堆关于$v_\pi(s\prime)$的加权求和。

因此，我们可以直接将这个式子无限延展之后写成下面那个求和的式子，并将$v_\pi(s\prime)$用最上面那个式子代替。如果带上$\gamma$,那么式子应该是
$$
\bigtriangledown V_\pi(s) = \sum_{x \in \mathbb{S}}\sum_{k=0}^{\infty} Pr(s->x,k,\pi)*\gamma^k* \bigtriangledown V_\pi(s) \\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ = \sum_{x \in \mathbb{S}}\sum_{k=0}^{\infty} Pr(s->x,k,\pi)*\gamma^k* \bigtriangledown \pi(a|x)*q_\pi(x,a)
$$
再利用之前马尔可夫平稳状态分布的式子，将其归一化处理变为马尔可夫平稳状态概率分布，因为我们不关心梯度的scale 他总是能被学习率控制.
$$
\sum_{k=0}^{\infty} Pr(s->x,k,\pi) = n(s) \propto \eta(s)
$$
最终我们的梯度$J(\theta)$也就是$V_\pi(s_0)$
$$
\bigtriangledown V_\pi(s_0) \propto \sum_{x \in \mathbb{S}}\eta(s)\sum_a \bigtriangledown\pi(a|s)q_\pi(s,a)
$$
从这个式子，我们可以发现，策略梯度的更新和起始点其实并没有什么关系，我们的更新式子中影响最大的还是我们平稳马尔可夫的分布，策略以及动作价值函数。

再看一眼这个式子，这个式子表达的其实是一个期望（概率乘上某一个关于自身的函数的乘积。所以我们可以将其写成期望的形式去掉求和号
$$\begin{aligned}
\bigtriangledown V_\pi(s_0) &\propto \sum_{x \in \mathbb{S}}\eta(s)\sum_a \bigtriangledown\pi(a|s)q_\pi(s,a) \\
&= \mathbb{E}_\pi[\ \sum_a \bigtriangledown\pi(a|S_t)q_\pi(S_t,a)\ ]  \\
&= \mathbb{E}_\pi[\ \sum_a \pi(a|S_t) \frac{\bigtriangledown\pi(a|S_t)}{\pi(a|S_t)}q_\pi(S_t,a)\ ]  \\
&= \mathbb{E}_\pi[\ \sum_a \pi(a|S_t) \frac{\bigtriangledown\pi(a|S_t)}{\pi(a|S_t)}q_\pi(S_t,a)\ ]  \\
&我们注意到\sum_a \pi(a|S_t)又是一个期望 \\
&用期望的原因是我们可以通过采样无偏估计期望 \\
&怎么估计呢 :蒙特卡洛方法\\
&= \mathbb{E}_\pi[\frac{\bigtriangledown\pi(A_t|S_t)}{\pi(A_t|S_t)}q_\pi(S_t,A_t)\ ]  \\
\end{aligned}
$$
在关注一下我们$\frac{\bigtriangledown\pi(A_t|S_t)}{\pi(A_t|S_t)}$,这个其实是$\theta$对$log(\pi(A_t|S_t))$的导数
$$
\bigtriangledown log \pi(A_t|S_t)*q_\pi(S_t|A_t)
$$
然后,根据$q_\pi(s,a)$的定义$\mathbb{E}_\pi[G_t]$,可以直接代入，最终表达式就是
$$
\mathbb{E}_\pi[G_t \bigtriangledown log \pi(A_t|S_t)]
$$
最终得到的参数更新式子
$$
\theta_{t+1}=\theta_t+\alpha G_t \bigtriangledown log \pi(A_t|S_t;\theta_t)
$$
特别的我们不加证明的给出带有$\gamma$的更新为
$$
G = \sum_{k=t+1}^T \gamma^{k-t-1} R_k \\
\theta_{t+1}=\theta_t+\alpha \gamma^tG \bigtriangledown log \pi(A_t|S_t;\theta_t)
$$
其中我们的 S,A,R 都是蒙特卡洛采样所得到的完整的trajectory($S_0,A_0,R_1$....,$S_{T-1},A_{T-1},R_T$)
这是一个off-policy的。
### baseline 方法
纯粹的蒙特卡洛采样方法我们在之前介绍的时候曾经提到过方差是非常大的，因为我们实际问题的状态和动作空间会非常巨大，这就导致我们采样的方差会非常巨大。其实蒙特卡洛方法就没有方差不大的。
于是我们引入一个base line，这一点也在原先的论文中有所提及，他们会减去原始的均值，也是为了减少方差。

b(s)表示我们某一个baseline策略(该策略可以是任何关于s的函数，只要他是和我们的a无关的)在状态s上的价值函数，加上baseline的形式为
$$
\bigtriangledown J_\pi(\theta) \propto \sum_{x \in \mathbb{S}}\eta(s)\sum_a \bigtriangledown\pi(a|s)(q_\pi(s,a)-b(s))
$$
而对于上式，因为b和a无关 提到前面，再交换求梯度和求和号，求和号的总和为1（概率分布的总和为1），因此梯度为0.
所以我们加入一个baseline对原来的式子不会有任何影响
$$\begin{aligned}
\sum_a b(s)\bigtriangledown \pi(a|s,\theta)&=b(s)\bigtriangledown \sum_a\pi(a|s,\theta) \\
&=b(s)\bigtriangledown1 \\ &=0
\end{aligned}
$$
那么我们的参数更新也会发生相应的变化：
$$
\theta_{t+1}=\theta_t+\alpha (G_t-b(S_t)) \bigtriangledown log \pi(A_t|S_t;\theta_t)
$$
下面提供了一个很经典的方法去选择我们的b(s)，利用对状态价值函数的估计来完成。我们假设每一个状态价值函数可以被参数化为$\hat v(s,\mathbf{w})$,我们在更新$\theta$的时候也更新这个$\mathbf{w}$，有以下算法
![1](/图片/22.png)

### actor-critic
回忆之前的TD方法,我们在TD方法中使用了bootstrap和n-step来实现forward和backward。从TD算法中取得灵感，我们不加证明的给出下面的两个算法，分别为单步AC和AC with ET
![1](/图片/24.png)  
![1](/图片/25.png)
上述两个算法是totally online and incremental

### 连续环境
连续环境中我们需要用每一个时间的平均奖励来描述我们的策略表现。但是实际上，更新的公式是差不多的。
![1](/图片/26.png)

这里的$\mu(s)$就是之前说的马尔可夫的平稳分布，在马尔可夫的平稳分布中，我们假设平稳分布和初始状态是无关的（这其实是马尔可夫的细致平稳条件），细致平稳条件下的马尔可夫链一旦确定了动态特性，只要动作是动态特性内部定义的，就不会脱离这个平稳分布。

下图描述了基于上述理论展开的伪代码，是on-policy的。
![1](/图片/27.png)
伪代码中的 $\bar R$是奖励的均值，这个在论文中也有体现。放在这里的作用我觉得是为了平滑曲线，减小方差，加快训练速度。

